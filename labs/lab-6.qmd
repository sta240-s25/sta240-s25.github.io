---
title: "Lab 6 - simulation"
subtitle: Due Thursday March 20 at 11:59 PM
---

# Part 1: quantiles

The **quantile function** of a random variable is the (*generalized*) *inverse* of its cdf. So $F^{-1}$. It is the function that, given a probability $\alpha\in(0,\,1)$, returns the cut-off point on the number line that has that much probability *to the left*. So the $\alpha$th quantile $q_{\alpha}$ satisfies $P(X\leq q_\alpha) = \alpha$ and $F^{-1}(\alpha)=q_\alpha$. Here is the picture:

![](quantile.png){fig-align="center"}

The **median** is the $\alpha=0.5$ quantile. The quantiles of the standard normal distribution are famous:

```{r}
qnorm(0.95)
qnorm(0.975)
qnorm(0.995)
```

Every distribution family has a `q-` function that, given $p\in(0,\,1)$, returns the quantile $F^{-1}(p)=q_p$:

```{r}
#| eval: false
# discrete 

qbinom(p, size, prob) # Bernoulli is just binomial with size = 1
qgeom(p, prob)
qpois(p, lambda)

# continuous

qnorm(p, mean = 0, sd = 1)
qgamma(p, shape, rate = 1)
qunif(p, min = 0, max = 1)
qcauchy(p)
```

::: {.callout-warning collapse="true"}
## What about the discrete case?

In the discrete case, the CDF is a piecewise constant step function. Definitely not invertible. And in general, the cdf $F$ is not invertible, like in this picture:

![](quantile-fxn.png){width="50%" fig-align="center"}

So what do we mean by $F^{-1}$ then? That's when we invoke the *generalized* inverse and define 

$$
F^{-1}(\alpha)=\inf\{x\in\mathbb{R}:\alpha\leq F(x)\}.
$$

"inf" stands for *infimum*, which in mathematics is a special generalization of the minimum. **You don't have to know about any of this, by the way.**
:::

## Problem 1

Let $X\sim \text{Cauchy}$, meaning its cdf is 

$$
F_X(x)=\frac{\tan^{-1}x}{\pi}+\frac{1}{2},\quad x\in\mathbb{R}.
$$

a.  Find the inverse cdf $F_X^{-1}(y)$, $y\in(0,\,1)$;
b.  Use your inverse cdf formula to compute the 60% quantile of $X$;
c.  Use the `qcauchy` function to compute the same quantile, and verify that you get the same number.

## Problem 2

Let $X\sim\text{Gamma}(1,\,\beta)$. So the pdf of $X$ is

$$
f_X(x)=\beta e^{-\beta x},\quad x>0.
$$

a.  Find the cdf $F_X(x)=P(X\leq x)$;
b.  Find the inverse cdf $F_X^{-1}(y)$, $y\in(0,\,1)$;
c.  Use your inverse cdf formula to compute the median of $X$. It will be a function of $\beta$;
d.  Fix $\beta = 2$. Use the `qgamma` function to compute the median of $X$, and verify that you get the same number as you get when you use your formula from part c. 

# Part 2: simulation

When you call the `r-` functions and get random numbers from some distribution, what is the computer doing under the hood? Ultimately, it's simulating simpler stuff and then applying a transformation to give you the distribution that you want. For example...

-   Want Cauchy? Simulate $\text{Unif}(-\pi/2,\,\pi/2)$ and apply tan;
-   Want Gamma(1/2, 1/2)? Simulate N(0, 1) and square;
-   Want $\text{N}(\mu,\,\sigma^2)$? Simulate N(0, 1) and then scale and shift.

::: callout-warning
If you have no clue what any of that just meant, please take a second look at [Lab 5](https://sta240-s25.github.io/labs/lab-5.html).
:::

Okay, but where is the simple stuff coming from? It turns out that you can boil everything down to the standard uniform distribution $\text{Unif}(0,\, 1)$.

::: callout-note
## Theorem: inverse transform sampling

Let $U\sim \text{Unif}(0,\, 1)$, and let $F$ be any cdf (non-decreasing, right-continuous, yada yada) you're interested in. If you define a new random variable $X=F^{-1}(U)$, then $X\sim F$.
:::

::: {.callout-tip collapse="true"}
## Proof
For simplicity, assume $F$ is smooth and invertible. Things still work if it isn't, but it makes the math a little cleaner. Furthermore, recall the cdf of the standard uniform $U\sim\text{Unif}(0,\, 1)$:
$$
    F_U(x)=\begin{cases}
        0 & x \leq 0\\
        x & 0<x<1\\
        1 & 1 \leq x.
    \end{cases}
$$

$X$ is just a transformation of $U$, so we can apply the cdf method. 
$$
    \begin{align*}
        F_X(x)
        &=
        P(X\leq x)
        \\
        &=
        P(F^{-1}(U)\leq x)
        \\
        &=
        P(U\leq F(x))
        \\
        &=
        F_U\left(F(x)\right)
        \\
        &=
        F(x)
        .
    \end{align*}
$$

So, the cdf of $X$ is literally just $F$. 
:::

In words, this theorem is saying that you can simulate *any* distribution by simulating the standard uniform distribution and then applying a transformation. We statisticians take for granted that the computer scientists figured out how to simulate $\text{Unif}(0,\, 1)$ ([here](https://en.wikipedia.org/wiki/Mersenne_Twister) is an example), and so if we want random numbers from some fancy distribution, we just need to figure out what the transformation is.

## Problem 3

If you took AP Statistics or STA 101, you probably learned about Student's $t$ distribution. This is a family of continuous distributions with heavier tails than the normal, and the density is:

$$
f_X(x;\,\nu)=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\sqrt{\pi\nu}}\left(1+\frac{1}{\nu}x^2\right)^{-\frac{\nu+1}{2}},\quad x\in\mathbb{R}.
$$

This family has one parameter $\nu>0$ called the *degrees of freedom*. Okay, that's a big rotten mess, but whatever. If we wanted to simulate it, all we have to do is simulate from $\text{Unif}(0,\, 1)$, and then plug those numbers into the inverse cdf of the $t$ distribution. The transformed numbers are guaranteed to follow the $t$ distribution. Let's see that!

Student's $t$ has it's own `d-` and `q-` functions:

```{r}
#| eval: false
dt(x, df)
qt(p, df)
```

`dt` evaluates the pdf, and `qt` evaluates the *inverse* cdf (ie the quantile function). Implement the following:

-   Simulate $n=5000$ numbers from $\text{Unif}(0,\, 1)$;
-   Plug them into the inverse cdf of the $t$ distribution with `df = 10`;
-   Plot a histogram of these transformed numbers;
-   Add on top of the histogram a line plot of the density of the $t$ distribution with ten degrees of freedom;
-   They had better match!

## Problem 4

Let $X\sim\text{Gamma}(1,\,\beta)$. So the pdf of $X$ is

$$
f_X(x)=\beta e^{-\beta x},\quad x>0.
$$

a.  Use the work you did in Part 1 to implement inverse transform sampling to simulate $\text{Gamma}(1,\,\beta)$ for $\beta = 2$.
b.  Plot a histogram of 5000 draws from your sampler, and add a line plot of the $\text{Gamma}(1,\,\beta)$ density on top. They had better match!

## Problem 5

Let $X\sim \text{Cauchy}$. Use the work you did in Part 1 to implement inverse transform sampling for the Cauchy distribution. Plot a histogram of 5000 draws from your sampler, and add a line plot of the Cauchy density on top. They had better match!

## Problem 6

The bottom line of all of this is the **universality of the uniform**. The fundamental principle of simulation is that *any* distribution can be simulated by first simulating from the standard uniform, and then applying a transformation.

The **Box-Muller** method says that we can simulate the standard normal in the following way:

$$
\begin{align*}
U_1,\,U_2&\overset{\textrm{iid}}{\sim}\textrm{Unif}(0,\,1)\\
Z&=\sqrt{-2\ln U_1}\cos(2\pi U_2)
\end{align*}
$$

So $Z\sim\text{N}(0,\,1)$. It is not hard to prove this if you've taken multivariable calculus (yaaay polar coordinates), but our course does not assume this. Nevertheless, we can still implement the sampler and check that it works.

a.  Use the Box-Muller method to simulate 5000 random numbers from N(0, 1). Check that you did it right by plotting the histogram and superimposing the standard normal density;
b.  Modify your code to simulate $\text{N}(\mu=-3.5,\,\sigma^2=0.58)$, and compare a histogram with a density to verify that it works.
