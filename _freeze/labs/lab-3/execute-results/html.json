{
  "hash": "af4dbd40f6705e474e47162bd24eead0",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lab 3 - How does ChatGPT generate the next word?\"\nsubtitle: Due Thursday February 6 at 11:59 PM\n---\n\n\n# Part 1: vector refresher\n\nA *vector* in R is just an ordered set of values. The easiest way to create one is to manually list out the values, separated by commas, inside `c()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmyvec <- c(pi, 5, 3.6, 2, 9, 6000) \nmyvec\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]    3.141593    5.000000    3.600000    2.000000    9.000000 6000.000000\n```\n\n\n:::\n:::\n\n\nIf you want to list out all of the integers between some min and max, you can use this shortcut:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n5:15\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1]  5  6  7  8  9 10 11 12 13 14 15\n```\n\n\n:::\n:::\n\n\nMore generally, if you want a vector of evenly spaced numbers between a min and a max, do this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na <- seq(0, 1, length.out = 11) \na\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n```\n\n\n:::\n:::\n\n\nIf you want to create a \"blank\" vector that just has zeros in it, here you go:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nz <- numeric(10)\nz\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] 0 0 0 0 0 0 0 0 0 0\n```\n\n\n:::\n:::\n\n\nIn all of those cases, our vector contained numbers, but there is nothing special about numbers. Here is a vector where each value is a *string* (a piece of text):\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoetry <- c(\"Mary\", \"had\", \"a\", \"little\", \"chainsaw\")\npoetry\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Mary\"     \"had\"      \"a\"        \"little\"   \"chainsaw\"\n```\n\n\n:::\n:::\n\n\nIn [Lab 1](https://sta240-s25.github.io/labs/lab-1.html) you learned about the `sample` command. This can be used to simulate a random phenomenon with a finite sample space. To simulate five flips of a fair coin:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(x = c(\"H\", \"T\"), size = 5, replace = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"H\" \"T\" \"H\" \"T\" \"H\"\n```\n\n\n:::\n:::\n\n\nThe vector `c(\"H\", \"T\")` represents the sample space, `size = 5` is the number of simulations you want to perform, and `replace = TRUE` ensures that you are repeating the trial from scratch each time. By default, `sample` assumes that all outcomes are equally likely. To override this, you can supply a vector of probabilities:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(x = c(\"H\", \"T\"), size = 5, replace = TRUE, prob = c(0.8, 0.2))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"T\" \"H\" \"T\" \"H\" \"H\"\n```\n\n\n:::\n:::\n\n\nYou see that the vector is the same length as the sample space (so every outcome gets its own individual probability), and the probabilities have to sum to one.\n\n# Part 2: how does ChatGPT generate the next word?\n\nIf you've ever asked [ChatGPT](https://chatgpt.com/) a question, you may have noticed something: it generates its response one word at a time right there in front of you.\n\n![](chatgpt.gif){fig-align=\"center\" width=\"70%\"}\n\nWhat exactly is it doing? Well, [ask it](https://chatgpt.com/share/677ef289-9564-8008-a662-9f8c2cfb04fc)! If you do, the explanation will go something like this...\n\nChatGPT is an example of a *large language model* (LLM). Under the hood, there's this fancy pants deep neural network machine learning mumbo jumbo that has been trained on a massive corpus of documents (books, newspaper articles, academic publications, all of Wikipedia, etc). When you ask it a question like \"What color is the apple?\", the model asks itself \"*conditional* on the text you just gave me, what is the likely next word?\" Based on the text it was trained on and the underlying ML model, ChatGPT constructs a *conditional probability distribution* over the set of \"next words\" and simulates from it, like with the `sample` function. The new word that it simulates gets added to the end of the text you provided, and it repeats the process over again: given where we are *now*, what is the next word likely to be? And on and on...\n\nSo for example, you ask it \"What color is the apple?\" Based on that, the model constructs a probability distribution for $P(\\text{next word}\\,|\\,\\text{\"What color is the apple?\"})$. The sample space $S$ here is (basically) the set of all words that the model has seen. That set could be pretty big, but nevertheless it is finite. So we just need to list out all of the possible words together with their individual probabilities:\n\n| Next word | Conditional probability |\n|-----------|-------------------------|\n| red       | 0.1                     |\n| is        | 0.05                    |\n| the       | 0.2                     |\n| apple     | 0.08                    |\n| amphibian | 0.01                    |\n| green     | 0.05                    |\n| Zito      | 0.00                    |\n| *etc*     | *etc*                   |\n\nOnce this is constructed, the model simulates from the distribution. You could imagine that the code might look something like `sample(words, size = 1, prob = conditional_probs)`. So say that happens and the next word we simulate is \"the.\" That gets added to the end, and now we want to know $P(\\text{next word}\\,|\\,\\text{\"What color is the apple? the\"})$. So we query the underlying model, and it constructs a new distribution on words, conditional on this revised context:\n\n| Next word | Conditional probability |\n|-----------|-------------------------|\n| red       | 0.09                    |\n| is        | 0.04                    |\n| the       | 0.001                   |\n| apple     | 0.21                    |\n| amphibian | 0.01                    |\n| green     | 0.05                    |\n| Zito      | 0.00                    |\n| *etc*     | *etc*                   |\n\nNotice that the information we are conditioning on has changed, so the probabilities change. Based on the new probabilities, we simulate the next word. Let's say it ends up being \"apple.\" So we add that to the end, and ask the model to construct a new distribution for $P(\\text{next word}\\,|\\,\\text{\"What color is the apple? the apple\"})$:\n\n| Next word | Conditional probability |\n|-----------|-------------------------|\n| red       | 0.07                    |\n| is        | 0.5                     |\n| the       | 0.001                   |\n| apple     | 0.00                    |\n| amphibian | 0.01                    |\n| green     | 0.09                    |\n| Zito      | 0.00                    |\n| *etc*     | *etc*                   |\n\nNow we simulate \"is,\" add that to the end, and query the model for $P(\\text{next word}\\,|\\,\\text{\"What color is the apple? the apple is\"})$:\n\n| Next word | Conditional probability |\n|-----------|-------------------------|\n| red       | 0.35                    |\n| is        | 0.01                    |\n| the       | 0.001                   |\n| apple     | 0.001                   |\n| amphibian | 0.002                   |\n| green     | 0.2                     |\n| Zito      | 0.00                    |\n| *etc*     | *etc*                   |\n\nWe see at this point that \"red\" is the most likely next word, but we are not searching the table for the most likely next word and slavishly returning it. We are simulating. So there's a 35% chance we'll come up with \"red,\" but also a non-trivial possibility that we'll come up with \"green.\" You will notice that if you ask ChatGPT the exact same question multiple times (literally the identical text prompt), it returns different answers. That is because of this element of randomness.\n\nObviously, the elephant in the room in the above discussion is \"where did the probabilities come from?\" This is the magic, such as it is, of LLMs. In order to implement this scheme, we need to be able to construct a conditional probability distribution on words given *any* context. So no matter if you give it \"What color is the apple\" or \"Explain string theory like I'm a genius\" or \"Why did my wife leave me\" or \"que se passe t'il,\" it needs to come up with a meaningful and useful distribution for the next word, and it has to repeat this thousands if not millions of times. There are so many possibilities that it's kind of bonkers to expect to be able to do this, but with enough data and computing power and a big enough model, apparently we can pull it off (at least some of the time).\n\nSo, the question of \"where did the probabilities come from?\" is sadly beyond the scope of our course. However, the question of \"what do you do with the probabilities once you have them?\" is perfectly accessible to us, even at this early stage, so let's have some fun...\n\n# Part 3: now you try\n\nThe [$k$-gram language model](https://en.wikipedia.org/wiki/Word_n-gram_language_model) is I guess what you'd call a \"small language model.\" Next to ChatGPT, it looks prehistoric. But it is simple and can be comfortably run on a laptop. We have trained this SLM on a single document: the text of Shakespeare's *A Midsummer Night's Dream*. Given the pre-trained model we are handing you, we ask that you implement the recursive scheme above in order to generate a text response, the same way ChatGPT does.\n\nIn all cases, the starting `context` is\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncontext <- \"now fair hippolyta\"\n```\n:::\n\n\nHippolyta is Queen of the Amazons in the play. Let's see if our model can spit game.\n\n## Task 1\n\nWe have provided you with a pre-trained language model `kn`, a vector `midsummer_words` containing the 3,063 unique words in the play, and a function `next_word_probs`:\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncond_dist <- next_word_probs(context, midsummer_words, kn)\ntail(cond_dist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         words         prob\n3058  unearned 3.375323e-05\n3059    'scape 3.375323e-05\n3060 serpent's 3.375323e-05\n3061    amends 6.751470e-05\n3062      liar 3.375323e-05\n3063   restore 3.375323e-05\n```\n\n\n:::\n:::\n\n\nSo, given the context, the set of possible words, and the model, it returns a table (a *data frame*) listing out all the words together with their *conditional probabilities* given the context.\n\nRun this function and use the output together with the `sample` function to simulate a new word.\n\n## Task 2\n\nUse the `paste` function to append the word you simulated in Task 1 to the end of `context`. Then, starting from this updated context, go back to the beginning, generate a new conditional distribution, and use it to simulate a new word.\n\n## Task 3\n\nWrite a for loop that iterates the process in Tasks 1 and 2 in order to add six new words to the context.\n\n## Task 4\n\nRerun your code ten times to generate ten different responses. How many of them would you consider *basically* coherent?\n\n## Task 5\n\nNow that you have the basic code, play around with different starting contexts besides \"now fair hippolyta.\" Bear in mind that our world is small here. We've trained the model on a single, old text, so if you want coherent responses, you should probably give it something in the ballpark of what it has seen before. I doubt \"which banger slaps hardest my dude\" will yield anything meaningful (though feel free to try!). For inspiration, [here](https://shakespeare.mit.edu/midsummer/full.html) is the full text of the original play.\n\n## Task 6\n\nUp to now we've been worrying a lot about individual words, but individual words don't create meaning. It's how those words interact across phrases and sentences and paragraphs. Unfortunately, we do not have methods for inducing a probability distribution over the set of all possible sentences, or the set of all possible thoughts. But in a sense, language models are attempting to *approximate* such a thing. And on the evidence to date, maybe they succeed sometimes. Anyway, how do we go from a method that gets things right on the word level to a method that gets things right on the sentence level? Well, therein lies the art of the engineers that work on these things, and one method they have found helpful is called *temperature sampling*.\n\nSo far, given the current context, the model returns a (conditional) probability $p(i)$ for each word $i\\in \\text{corpus}$. So far we've used these probabilities unmodified. With temperature sampling, before we draw the next word, we would first adjust the probabilities based on a user-defined tuning parameter $t>0$ called the *temperature*:\n\n$$\np_t(i) = \\frac{e^{\\ln\\frac{p(i)}{t}}}{\\sum\\limits_{i\\in\\text{corpus}}e^{\\ln\\frac{p(i)}{t}}},\\quad i\\in\\text{corpus}.\n$$\n\nIf $t=1$, we're back to the original probabilities (check this!). Otherwise, to quote the $k$-grams people, \"higher and lower temperatures make the original probability distribution smoother and rougher, respectively. By making a physical analogy, we can think of less probable words as states with higher energies, and the effect of higher (lower) temperatures is to make more (less) likely to excite these high energy states.\" Why do people do this? To the best of my knowledge, it is no deeper than \"it seems to work well.\" In practice, adjusting the temperature seems to have a macro effect on the overall character of the *sentences*.\n\nThe function `next_word_probs` that we gave you has a fourth argument `temp` where you can give it a number and it will give you the *tempered* probabilities. Play around with big and small values, and report any effect that it has on the overall quality of the sentences.\n\n## Task 7\n\nCode temperature sampling yourself. Take the probabilities that `probs <- next_word_probs(...)` gives you when `temp = 1.0`, and then use basic R commands like `exp`, `log`, etc to apply the formula above and compute a vector `temp_probs` with the revised numbers for `temp = 0.5`. Check that your numbers agree with the ones that our function `next_word_probs` gives when you supply `0.5` as the fourth argument.\n\n# Part 4: further reading\n\nIf you're interested in this stuff...\n\n-   The guy who created WolframAlpha has a nice [primer](https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/);\n-   People seemed to like [this video](https://youtu.be/zjkBMFhNj_g?feature=shared) when it made the rounds a few years back;\n-   The $k$-grams model might serve as \"baby's first language model.\" It looks naive compared to the LLMs of today, but in order to understand how it works, you have to confront ideas like tokenization and temperature sampling, which apply to many language models, large or small. [This](https://cran.r-project.org/web/packages/kgrams/vignettes/kgrams.html) is the `R` package that we used for this lab.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}